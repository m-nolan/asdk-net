{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads all packages at beginning\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = getpass.getuser()\n",
    "if username == 'mickey':\n",
    "    data_directory = '/Volumes/Samsung_T5/aibs_data/example_ecephys_project_cache/'\n",
    "    save_directory = '/Volumes/Samsung_T5/aibs_data/savedir/'\n",
    "elif username == 'rahul':\n",
    "    data_directory = 'D:\\\\OneDrive - UW\\\\research\\\\data@aibs' # must be updated to a valid directory in your filesystem\n",
    "    save_directory = 'D:\\\\OneDrive - UW\\\\research\\\\data@aibs\\\\savedir'\n",
    "elif username == 'Ryan':\n",
    "    data_directory = 'C:\\\\Users\\\\Ryan\\\\Documents\\\\Grad School\\\\Research\\\\Eli\\\\Group\\\\Hack\\\\Allen_Data' # must be updated to a valid directory in your filesystem\n",
    "    save_directory = 'C:\\\\Users\\\\Ryan\\\\Documents\\\\Grad School\\\\Research\\\\Eli\\\\Group\\\\Hack\\\\Save_Data'\n",
    "    \n",
    "    \n",
    "manifest_path = os.path.join(data_directory, \"manifest.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions[sessions.session_type == 'functional_connectivity'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sessions and corresponding data\n",
    "sessions = cache.get_session_table()\n",
    "probes = cache.get_probes()\n",
    "channels = cache.get_channels()\n",
    "units = cache.get_units()\n",
    "\n",
    "\n",
    "#print('Total number of sessions: ' + str(len(sessions)))\n",
    "#sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions.ecephys_structure_acronyms.values[44]\n",
    "# sessions[sessions.session_type == 'functional_connectivity'].index.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A_Config:\n",
    "    def __init__(self, data_dir, save_dir, sess_id, stimulus):\n",
    "        \n",
    "        self.data_dir = data_dir \n",
    "        self.save_dir = save_dir\n",
    "        self.sess_id = sess_id\n",
    "        self.stimulus = stimulus\n",
    "\n",
    "        self.name = os.path.join(self.save_dir,\"ID{}_{}\".format(self.sess_id, self.stimulus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_Ps(cache,sess_id,stim_label_list,bin_size):\n",
    "    data = cache.get_session_data(sess_id)\n",
    "    #t_bin = {}\n",
    "    #P = {}\n",
    "    for i, stim_label in enumerate(stim_label_list):\n",
    "        allencon = A_Config(data_directory,save_directory,sess_id,stim_label)\n",
    "        print(i)\n",
    "        # get start, end time points\n",
    "        stim_data_table = data.get_stimulus_table()[data.get_stimulus_table().stimulus_name==stim_label]\n",
    "        start_time = np.array(stim_data_table.start_time)\n",
    "        stop_time = np.array(stim_data_table.stop_time)\n",
    "        trial_time = np.mean(stop_time - start_time)\n",
    "        bin_edges = np.arange(trial_time + bin_size, step = bin_size )\n",
    "        t_bin = bin_edges\n",
    "        print(i)\n",
    "        stim_presentation_ids = data.stimulus_presentations.loc[\n",
    "            (data.stimulus_presentations['stimulus_name'] == stim_label)\n",
    "            ].index.values\n",
    "        print(i)\n",
    "        unit_ids = data.units.index.values\n",
    "        print(i)\n",
    "        P = data.presentationwise_spike_counts(bin_edges,stim_presentation_ids,unit_ids)/bin_size\n",
    "        pkl.dump(P,open(allencon.name+'_bin_{}_P.p'.format(bin_size),'wb'))\n",
    "        pkl.dump(t_bin,open(allencon.name+'_bin_{}_t.p'.format(bin_size),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(mat,motif,id):\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "    norm = mpl.colors.Normalize(vmin=0.,vmax=np.max(mat))\n",
    "    img=ax.matshow(mat,cmap='RdBu_r',norm=norm)\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    cbar_ax.tick_params(labelsize=20)\n",
    "    fig.colorbar(img,cax=cbar_ax)\n",
    "    ax.set_ylabel(\"Neuron index\",fontsize=20)\n",
    "    ax.set_xlabel(\"Neuron index\",fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    fig.savefig(id+'_'+str(motif)+'.pdf',format='pdf',dpi=600)\n",
    "    fig.savefig(id+'_'+str(motif)+'.png',format='png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_spikes( P, kernel_width , bin_size ):\n",
    "    # Rahul\n",
    "    \"\"\"\n",
    "    P: #trials x #units x #time-points sized ndarray\n",
    "    kernel_param: width of kernel (in samplingrate*1000 units)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import time\n",
    "    import scipy.stats, scipy.signal\n",
    "    import scipy as sp\n",
    "\n",
    "    start = time.time() # tic Measures function running speed. Works only on Macs\n",
    "\n",
    "    units = P.shape[0]\n",
    "    trials = P.shape[1]\n",
    "    bins = P.shape[2]\n",
    "\n",
    "    # Create fake data for testing purposes\n",
    "    #    array = np.random.randint(0,2,(trials,bins))\n",
    "    #    sigma = 45\n",
    "\n",
    "    #Define kernel\n",
    "    kernel_param = kernel_width/bin_size\n",
    "    sigma = kernel_param/6.\n",
    "    edges = np.arange(-3*sigma,3*sigma+1,1)\n",
    "    kernel = sp.stats.norm.pdf(edges,0, sigma) #Use a gaussian function\n",
    "    kernel = kernel/sum(kernel)\n",
    "    \n",
    "    #Compute Spike Density Function for all trials\n",
    "    #X = np.zeros((trials,units,bins))\n",
    "    X = sp.signal.convolve(P, kernel[None,:, None], mode='same',method='direct')\n",
    "\n",
    "    print('Run time for SDF function was ' + str(round(time.time()-start)) + 'seconds') # toc\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Calculations (Run for each stimulus/session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_id=791319847\n",
    "stim_label_list = ['natural_scenes','drifting_gratings_contrast','drifting_gratings'] #All stimuli used\n",
    "stim_label_list_idx = 2\n",
    "stim_label = stim_label_list[stim_label_list_idx]\n",
    "\n",
    "allencon = A_Config(data_directory,save_directory,sess_id,stim_label)\n",
    "print(stim_label)\n",
    "print(allencon.stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bin & Kernel Parameters\n",
    "bin_size=0.01\n",
    "kernel_width=0.5\n",
    "\n",
    "#kernel_param=kernel_width/bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_Ps(cache, sess_id, stim_label_list = [stim_label], bin_size=bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X={}\n",
    "for i, stim_label_ in enumerate([stim_label]):\n",
    "    print(i)\n",
    "    allencon = A_Config(data_directory,save_directory,sess_id,stim_label_)\n",
    "    P = pkl.load(open(allencon.name+'_bin_{}_P.p'.format(bin_size),'rb'))\n",
    "    X[stim_label] = smooth_spikes(P.values,kernel_width,bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[stim_label][0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "# for p_idx, k in enumerate([0,9,19,29]):\n",
    "#     ax = f.add_subplot(2,2,p_idx+1)\n",
    "#     plt.plot(X[stim_label][0,:,k])\n",
    "#     plt.xlabel('samples')\n",
    "#     plt.ylabel('firing rate')\n",
    "    \n",
    "plt.imshow(X[stim_label][0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(n_trial)\n",
    "for k in range(n_trial):\n",
    "    a[k] = np.mean(np.std(X[stim_label][k,:,:],axis=0)==0)\n",
    "\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trial = X[stim_label].shape[0]\n",
    "n_time = X[stim_label].shape[1]\n",
    "n_unit = X[stim_label].shape[2]\n",
    "dither_noise = 0.1\n",
    "adj_mats = []\n",
    "unit_list = []\n",
    "for k in range(1):\n",
    "    print(k)\n",
    "    t_ = time.time()\n",
    "    units_to_use_idx_ = np.std(X[stim_label][k,:,:],axis=0)!=0\n",
    "    model = GraphicalLassoCV()\n",
    "    model.fit(X[stim_label][k,:,units_to_use_idx_])\n",
    "    cov_ = model.covariance_\n",
    "    prec_ = model.precision_\n",
    "    adj_mats.append(prec_)\n",
    "    unit_list.append(units_to_use_idx_)\n",
    "    print('{}s'.format(time.time()-t_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_matrix(100*np.abs(prec_),'{}_{}_trial{}'.format(sess_id,stim_label,0),'ggm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[stim_label][k,:,units_to_use_idx_].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Calculations/Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(P[stim_label].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[stim_label] = smooth_spikes(P[stim_label].values,kernel_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[stim_label][0,0,:])\n",
    "plt.plot(X[stim_label][0,1,:])\n",
    "plt.plot(X[stim_label][0,2,:])\n",
    "plt.plot(X[stim_label][0,3,:])\n",
    "plt.plot(X[stim_label][0,4,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sess_trials(cache, sess_id, stim_label_list, bin_size, kernel_width):\n",
    "    # load session data\n",
    "    data = cache.get_session_data(sess_id)\n",
    "    t_bin = {}\n",
    "    P = {}\n",
    "    X = {}\n",
    "    for i, stim_label in enumerate(stim_label_list):\n",
    "        print(i)\n",
    "        # get start, end time points\n",
    "        stim_data_table = data.get_stimulus_table()[data.get_stimulus_table().stimulus_name==stim_label]\n",
    "        start_time = np.array(stim_data_table.start_time)\n",
    "        stop_time = np.array(stim_data_table.stop_time)\n",
    "        trial_time = np.mean(stop_time - start_time)\n",
    "        bin_edges = np.arange(trial_time + bin_size, step = bin_size )\n",
    "        t_bin[stim_label] = bin_edges\n",
    "        print(i)\n",
    "        stim_presentation_ids = data.stimulus_presentations.loc[\n",
    "            (data.stimulus_presentations['stimulus_name'] == stim_label)\n",
    "            ].index.values\n",
    "        print(i)\n",
    "        unit_ids = data.units.index.values\n",
    "        print(i)\n",
    "        P[stim_label] = data.presentationwise_spike_counts(bin_edges,stim_presentation_ids,unit_ids)/bin_size\n",
    "        print(i)\n",
    "        # get windowed spike times\n",
    "        # S[i] = window_spk_times(data,start_time,stop_time)\n",
    "        # get binned spike rates\n",
    "        # t_bin[i], P[i] = bin_spikes(S[i],bin_size)\n",
    "        # kernel smoothed estimate of binned spike rates\n",
    "        X[stim_label] = smooth_spikes(P[stim_label].values,kernel_width)\n",
    "        \n",
    "    return (t_bin, P, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = cache.get_session_data(sess_id)\n",
    "t_bin = {}\n",
    "#P = {}\n",
    "for i, stim_label in enumerate(stim_label_list):\n",
    "    allencon = A_Config(data_directory,save_directory,sess_id,stim_label)\n",
    "    print(i)\n",
    "    # get start, end time points\n",
    "    stim_data_table = data.get_stimulus_table()[data.get_stimulus_table().stimulus_name==stim_label]\n",
    "    start_time = np.array(stim_data_table.start_time)\n",
    "    stop_time = np.array(stim_data_table.stop_time)\n",
    "    trial_time = np.mean(stop_time - start_time)\n",
    "    bin_edges = np.arange(trial_time + bin_size, step = bin_size )\n",
    "    t_bin[stim_label] = bin_edges\n",
    "    print(i)\n",
    "    stim_presentation_ids = data.stimulus_presentations.loc[\n",
    "        (data.stimulus_presentations['stimulus_name'] == stim_label)\n",
    "        ].index.values\n",
    "    print(i)\n",
    "    unit_ids = data.units.index.values\n",
    "    print(i)\n",
    "    P = data.presentationwise_spike_counts(bin_edges,stim_presentation_ids,unit_ids)/bin_size\n",
    "    pkl.dump(P,open(allencon.name+'_bin_{}_P.p'.format(bin_size),'wb'))\n",
    "    print(i)\n",
    "    # get windowed spike times\n",
    "    # S[i] = window_spk_times(data,start_time,stop_time)\n",
    "    # get binned spike rates\n",
    "    # t_bin[i], P[i] = bin_spikes(S[i],bin_size)\n",
    "    # kernel smoothed estimate of binned spike rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P=pkl.load(open(allencon.name+'_bin_{}_P.p'.format(bin_size),'rb'))\n",
    "X = smooth_spikes(P.values,kernel_width=0.1, bin_size= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bin,P,X = load_sess_trials(cache, 794812542, stim_label_list = ['drifting_gratings_contrast'], bin_size=0.01, kernel_param=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dg=data.get_stimulus_table()[data.get_stimulus_table().stimulus_name=='drifting_gratings_contrast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.units.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.spike_times[data.units.index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_spikes=data.spike_times[data.units.index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_spikes[np.logical_and(unit_spikes>=data_dg.start_time.values[0],unit_spikes<data_dg.start_time.values[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_spikes( P, kernel_width , bin_size ):\n",
    "    # Rahul\n",
    "    \"\"\"\n",
    "    P: #trials x #units x #time-points sized ndarray\n",
    "    kernel_param: width of kernel (in samplingrate*1000 units)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import time\n",
    "    import scipy.stats, scipy.signal\n",
    "    import scipy as sp\n",
    "\n",
    "    start = time.time() # tic Measures function running speed. Works only on Macs\n",
    "\n",
    "    trials = P.shape[0]\n",
    "    units = P.shape[1]\n",
    "    bins = P.shape[2]\n",
    "\n",
    "    # Create fake data for testing purposes\n",
    "    #    array = np.random.randint(0,2,(trials,bins))\n",
    "    #    sigma = 45\n",
    "\n",
    "    #Define kernel\n",
    "    kernel_param = kernel_width/bin_size\n",
    "    sigma = kernel_param/6.\n",
    "    edges = np.arange(-3*sigma,3*sigma+1,1)\n",
    "    kernel = sp.stats.norm.pdf(edges,0, sigma) #Use a gaussian function\n",
    "    kernel = kernel/sum(kernel)\n",
    "    \n",
    "    #Compute Spike Density Function for all trials\n",
    "    #X = np.zeros((trials,units,bins))\n",
    "    X = sp.signal.convolve(P, kernel[None,None, :], mode='same',method='direct')\n",
    "\n",
    "    print('Run time for SDF function was ' + str(round(time.time()-start)) + 'seconds') # toc\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_width = 0.01\n",
    "sigma = kernel_width/6.\n",
    "edges = np.arange(-3*sigma,3*sigma+.001,.001)\n",
    "kernel = sp.stats.norm.pdf(edges,0, sigma) #Use a gaussian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_width = .01\n",
    "bin_size = .001\n",
    "kernel_param = kernel_width/bin_size\n",
    "sigma = kernel_param/6.\n",
    "edges = np.arange(-3*sigma,3*sigma+1,1)\n",
    "kernel = sp.stats.norm.pdf(edges,0, sigma) #Use a gaussian function\n",
    "kernel = kernel/sum(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(edges,kernel/sum(kernel))\n",
    "print(sum(kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_frate(P_stim, k_p):\n",
    "    \"\"\"\n",
    "    P_stim: #trials x #units x #time-points sized ndarray\n",
    "    k_p: width of kernel (in samplingrate*1000 units)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import time\n",
    "    import scipy.stats, scipy.signal\n",
    "    import scipy as sp\n",
    "\n",
    "    start = time.time() # tic Measures function running speed. Works only on Macs\n",
    "\n",
    "    trials = P_stim.shape[0]\n",
    "    units = P_stim.shape[1]\n",
    "    bins = P_stim.shape[2]\n",
    "\n",
    "    # Create fake data for testing purposes\n",
    "    #    array = np.random.randint(0,2,(trials,bins))\n",
    "    #    sigma = 45\n",
    "\n",
    "    #Define kernel\n",
    "    sigma = k_p/6. \n",
    "    edges = np.arange(-3*sigma,3*sigma+.001,.001)\n",
    "    kernel = sp.stats.norm.pdf(edges,0, sigma) #Use a gaussian function\n",
    "\n",
    "    #Compute Spike Density Function for all trials\n",
    "    Sdf = np.zeros((trials,units,bins))\n",
    "    Sdf = sp.signal.convolve(P_stim, kernel[None,None, :], mode='same',method='direct')\n",
    "\n",
    "    print('Run time for SDF function was ' + str(round(time.time()-start)) + 'seconds') # toc\n",
    "\n",
    "    return Sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_stim = np.random.rand(3,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stim=est_frate(P_stim,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(P_stim[0,0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_stim[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(P_stim[0,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_stim[0,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(P_stim[1,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_stim[1,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "def save_Ps(cache,sess_id,stim_label_list,bin_size):\n",
    "    data = cache.get_session_data(sess_id)\n",
    "    #t_bin = {}\n",
    "    #P = {}\n",
    "    for i, stim_label in enumerate(stim_label_list):\n",
    "        allencon = A_Config(data_directory,save_directory,sess_id,stim_label)\n",
    "        print(i)\n",
    "        # get start, end time points\n",
    "        stim_data_table = data.get_stimulus_table()[data.get_stimulus_table().stimulus_name==stim_label]\n",
    "        start_time = np.array(stim_data_table.start_time)\n",
    "        stop_time = np.array(stim_data_table.stop_time)\n",
    "        trial_time = np.mean(stop_time - start_time)\n",
    "        bin_edges = np.arange(trial_time + bin_size, step = bin_size )\n",
    "        t_bin = bin_edges\n",
    "        print(i)\n",
    "        stim_presentation_ids = data.stimulus_presentations.loc[\n",
    "            (data.stimulus_presentations['stimulus_name'] == stim_label)\n",
    "            ].index.values\n",
    "        print(i)\n",
    "        unit_ids = data.units.index.values\n",
    "        print(i)\n",
    "        P = data.presentationwise_spike_counts(bin_edges,stim_presentation_ids,unit_ids)/bin_size\n",
    "        pkl.dump(P,open(allencon.name+'_bin_{}_P.p'.format(bin_size),'wb'))\n",
    "        pkl.dump(t_bin,open(allencon.name+'_bin_{}_t.p'.format(bin_size),'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asdk_net",
   "language": "python",
   "name": "asdk_net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
